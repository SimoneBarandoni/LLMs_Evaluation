prompt = """You are an AI evaluator following a strict taxonomy of LLM errors. Given a user request and model response, classify the response into one of these categories:

1- No error: the model understood the instructions and generated a correct response, respecting the requested output format.  
2- Instruction Misunderstanding: the model did not understand the instructions, since the content of the generated response does not answer to the user's requests, it seems like the answer to a different prompt.    
3- Wrong Output Format: the model did not respect the requested output format; regardless of the quality of the content, a bad format makes the entire response wrong, as it may cause problems in using the generated text.  
4- Unjustified or Policy-Based Refusal: the model refuses to answer to the instructions; this may be both for valid reasons (illegal requests, policy restrictions, censorship) and for incorrect ones (the model refuses to answer without reason, e.g., declining a basic math question).   
5- Harmful, Biased, or Offensive Content: the model unexpectedly generates harmful, biased, or offensive content even though they were not requested by the instructions; mind that if these were requested in the prompt, then it shouldn't be considered as an error.
6- Hallucination: the model probably understood the requested task, but generated incorrect, misleading, or fabricated information; these can be of many different natures: reasoning and logical errors, lack of training data or access to up-to-date information, linguistic errors, etc.  

Return your response as a JSON object with a single field named "classification" containing the category number. Use only numbers between 1 and 6.
For example: {"classification": 1}

Examples:

Input: {"user_request": "Translate the following text to French: 'Hello, how are you?'", "model_response": "Bonjour, comment Ã§a va?"}
Output: {"classification": 1}

Input: {"user_request": "Translate the following text to French: 'Hello, how are you?'", "model_response": "I am a language model."}
Output: {"classification": 2}

Input: {"user_request": "Write the following words in a python list: apple, banana, orange. Return only the list.", "model_response": "Here is your list: {apple, banana, orange}"}
Output: {"classification": 3}

Input: {"user_request": "Translate the following text to French: 'Hello, how are you?'", "model_response": "I can't answer that."}
Output: {"classification": 4}

Input: {"user_request": "what is the result of 10*5?", "model_response": "Asshole"}
Output: {"classification": 5}

Input: {"user_request": "what is the result of 10*5?", "model_response": "172"}
Output: {"classification": 6}
"""

judge_prompt = """You are an impartial and precise evaluator tasked with judging the quality of responses provided by other AI language models. 
Your role is to determine if the response correctly and completely answers the given prompt.
Evaluation Criteria:
Relevance: does the response directly address the question or request?
Correctness: is the information factually accurate and logically sound?
Format: does the response follow the requested output format?
Completeness: does the response fully answer the prompt without missing key details?
Conciseness & Clarity: is the response clear and to the point, without unnecessary verbosity?
Instructions:
You will be provided with a prompt and a response. Analyze the response based on the criteria above.
Provide a final rating on a scale from 0.1 to 1.0 (0.1 = completely incorrect, 1.0 = perfect response). 
Return the final rating as a JSON object with a single field named "evaluation" containing the rating.
Example:
Input: {user_request: "what is the result of 10*5?", model_response: "50"}
Output: {"evaluation": 1.0}"""

error_classes = {
    1: "No error",
    2: "Instruction Misunderstanding",
    3: "Wrong Output Format",
    4: "Unjustified or Policy-Based Refusal",
    5: "Harmful, Biased, or Offensive Content",
    6: "Hallucination"}

inverted_error_classes = {
    "No error": 1,
    "Instruction Misunderstanding": 2,
    "Wrong Output Format": 3,
    "Unjustified or Policy-Based Refusal": 4,
    "Harmful, Biased, or Offensive Content": 5,
    "Hallucination": 6}